{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Preliminaries"
      ],
      "metadata": {},
      "id": "9179bacf-bc6e-4c76-be00-e389d68fc2f2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Python version\n",
        "import sys\n",
        "print(sys.version)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "3.8.20 (default, Oct  3 2024, 15:24:27) \n[GCC 11.2.0]\n"
        }
      ],
      "execution_count": 2,
      "metadata": {
        "gather": {
          "logged": 1745057699569
        }
      },
      "id": "84296ee4-106e-4f1a-9b6a-33a048b0911e"
    },
    {
      "cell_type": "code",
      "source": [
        "# import all modules\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import regex as re\n",
        "import random\n",
        "import spacy\n",
        "import hdbscan\n",
        "import pickle\n",
        "import dill\n",
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.decomposition import LatentDirichletAllocation as LDA\n",
        "from sklearn.decomposition import NMF\n",
        "from tmtoolkit.topicmod.evaluate import metric_coherence_gensim\n",
        "from nltk.corpus import stopwords\n",
        "from tqdm import tqdm\n",
        "from names_dataset import NameDataset\n",
        "from bertopic import BERTopic\n",
        "from umap import UMAP\n",
        "from sklearn.cluster import KMeans\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics import silhouette_score\n",
        "from IPython.core.magic import register_cell_magic"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "/anaconda/envs/clean38/lib/python3.8/site-packages/torch/cuda/__init__.py:654: UserWarning: Can't initialize NVML\n  warnings.warn(\"Can't initialize NVML\")\n/anaconda/envs/clean38/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n  from .autonotebook import tqdm as notebook_tqdm\n"
        }
      ],
      "execution_count": 3,
      "metadata": {
        "gather": {
          "logged": 1745057730565
        }
      },
      "id": "37d258c3-8a9c-4254-87c5-529f8460fc41"
    },
    {
      "cell_type": "code",
      "source": [
        "# # uncomment if not already donwloaded\n",
        "# nltk.download('stopwords')\n",
        "# nltk.download('punkt_tab')"
      ],
      "outputs": [],
      "execution_count": 4,
      "metadata": {
        "gather": {
          "logged": 1745057730882
        }
      },
      "id": "39d3ae87-d232-4813-9028-30ad4066e6a9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Set random seed \n",
        "seed = 1\n",
        "\n",
        "# Set global random seed  \n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "\n",
        "# Note: local random seed set below in\n",
        "# - sampling\n",
        "# - LDA\n",
        "# - UMAP"
      ],
      "outputs": [],
      "execution_count": 5,
      "metadata": {
        "gather": {
          "logged": 1745057731181
        }
      },
      "id": "a05e48f6-d985-4fbc-ac6b-e3cd4612d5e5"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function\n",
        "@register_cell_magic\n",
        "def skip(line, cell):\n",
        "    '''\n",
        "    Skip the cell\n",
        "    '''\n",
        "    return"
      ],
      "outputs": [],
      "execution_count": 6,
      "metadata": {
        "gather": {
          "logged": 1745057731769
        }
      },
      "id": "f9bcdaf9-b264-405b-afa8-a42320404398"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data Preparation"
      ],
      "metadata": {},
      "id": "03f6eff7-03b1-43b8-9520-fe4281574a23"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load and transform data (JSON to pandas df)"
      ],
      "metadata": {},
      "id": "96b3b787-491d-4504-a37a-20a6e269315d"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Currently, we use for each earnings call the full transcripts. Alternatively, we could use for each earnings call the transcript_splits (statements by individual speakers). The latter would require changes below under \"flatten nested data structure\"; the resulting df should be kept equivalent to the current version with the exception of having more rows (transcript_splits instead of full transcripts) to work with subsequent code. However, unlike in the current version, adjustments to accomodate differences in interpretations (topic per earnigns call transcript vs topic per transcript_split/ speaker) might be necessary.  "
      ],
      "metadata": {},
      "id": "42366861-bb97-4b5d-af2a-7bc450b05390"
    },
    {
      "cell_type": "code",
      "source": [
        "# load data from JSON files\n",
        "path = './'\n",
        "file = 'full_combined.json'\n",
        "\n",
        "with open(path + file, 'r') as f:\n",
        "    data = json.load(f)"
      ],
      "outputs": [],
      "execution_count": 7,
      "metadata": {
        "gather": {
          "logged": 1745057764766
        }
      },
      "id": "a8568db9-dc21-4c6a-bba7-697f7d525cfa"
    },
    {
      "cell_type": "code",
      "source": [
        "# flatten nested data structure\n",
        "records = []\n",
        "for year in data.keys(): \n",
        "    for quarter in data[year].keys():\n",
        "        for report in data[year][quarter]:\n",
        "            records.append({\n",
        "                'company name': report['company name'],\n",
        "                'ticker': report['ticker'],\n",
        "                'sector': report['sector'],\n",
        "                'industry': report['industry'],\n",
        "                'year': year,\n",
        "                'quarter': quarter,\n",
        "                'date': report['transcript']['date'],\n",
        "                'text': report['transcript']['transcript']\n",
        "            })"
      ],
      "outputs": [],
      "execution_count": 8,
      "metadata": {
        "gather": {
          "logged": 1745057765074
        }
      },
      "id": "fda47a80-b9e8-41df-9ea5-753a8601cac6"
    },
    {
      "cell_type": "code",
      "source": [
        "# create df\n",
        "df = pd.DataFrame(records)"
      ],
      "outputs": [],
      "execution_count": 9,
      "metadata": {
        "gather": {
          "logged": 1745057765371
        }
      },
      "id": "94a9cabc-7bb6-4f1c-a429-aac1c8a433b0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Exploratory data analysis"
      ],
      "metadata": {},
      "id": "5ad18f04-5376-4472-8106-a63d1f53c341"
    },
    {
      "cell_type": "code",
      "source": [
        "# View first entries\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 10,
          "data": {
            "text/plain": "             company name ticker                  sector  \\\n0    Agilent Technologies      A             Health Care   \n1              Apple Inc.   AAPL  Information Technology   \n2                  AbbVie   ABBV             Health Care   \n3  AmerisourceBergen Corp    ABC             Health Care   \n4     Abbott Laboratories    ABT             Health Care   \n\n                                     industry  year quarter        date  \\\n0              Life Sciences Tools & Services  2014      Q1  2014-02-13   \n1  Technology Hardware, Storage & Peripherals  2014      Q1  2014-01-27   \n2                               Biotechnology  2014      Q1  2014-04-25   \n3                    Health Care Distributors  2014      Q1  2014-04-24   \n4                       Health Care Equipment  2014      Q1  2014-04-16   \n\n                                                text  \n0  Executives: Bill Sullivan - President and CEO ...  \n1  Executives: Tim Cook - CEO Peter Oppenheimer -...  \n2  Executives: Richard Gonzalez – Chairman, Chief...  \n3  Operator: Greetings, and welcome to the CoreSi...  \n4  Operator: Good morning and thank you for stand...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>company name</th>\n      <th>ticker</th>\n      <th>sector</th>\n      <th>industry</th>\n      <th>year</th>\n      <th>quarter</th>\n      <th>date</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Agilent Technologies</td>\n      <td>A</td>\n      <td>Health Care</td>\n      <td>Life Sciences Tools &amp; Services</td>\n      <td>2014</td>\n      <td>Q1</td>\n      <td>2014-02-13</td>\n      <td>Executives: Bill Sullivan - President and CEO ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Apple Inc.</td>\n      <td>AAPL</td>\n      <td>Information Technology</td>\n      <td>Technology Hardware, Storage &amp; Peripherals</td>\n      <td>2014</td>\n      <td>Q1</td>\n      <td>2014-01-27</td>\n      <td>Executives: Tim Cook - CEO Peter Oppenheimer -...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AbbVie</td>\n      <td>ABBV</td>\n      <td>Health Care</td>\n      <td>Biotechnology</td>\n      <td>2014</td>\n      <td>Q1</td>\n      <td>2014-04-25</td>\n      <td>Executives: Richard Gonzalez – Chairman, Chief...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AmerisourceBergen Corp</td>\n      <td>ABC</td>\n      <td>Health Care</td>\n      <td>Health Care Distributors</td>\n      <td>2014</td>\n      <td>Q1</td>\n      <td>2014-04-24</td>\n      <td>Operator: Greetings, and welcome to the CoreSi...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Abbott Laboratories</td>\n      <td>ABT</td>\n      <td>Health Care</td>\n      <td>Health Care Equipment</td>\n      <td>2014</td>\n      <td>Q1</td>\n      <td>2014-04-16</td>\n      <td>Operator: Good morning and thank you for stand...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 10,
      "metadata": {
        "gather": {
          "logged": 1745057765703
        }
      },
      "id": "8ab87f2c-9d49-4167-a56b-7aaf1e7e4e25"
    },
    {
      "cell_type": "code",
      "source": [
        "# View last entries\n",
        "df.tail()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 11,
          "data": {
            "text/plain": "             company name ticker                  sector  \\\n20518          ExxonMobil    XOM                  Energy   \n20519         Yum! Brands    YUM  Consumer Discretionary   \n20520  Zebra Technologies   ZBRA  Information Technology   \n20521              Zoetis    ZTS             Health Care   \n20522       Zimmer Biomet    ZBH             Health Care   \n\n                                 industry  year quarter        date  \\\n20518                Integrated Oil & Gas  2024      Q4  2025-01-31   \n20519                         Restaurants  2024      Q4  2025-02-06   \n20520  Electronic Equipment & Instruments  2024      Q4  2025-02-13   \n20521                     Pharmaceuticals  2024      Q4  2025-02-13   \n20522               Health Care Equipment  2024      Q4  2025-02-06   \n\n                                                    text  \n20518  Jim Chapman: Good morning, everyone. Welcome t...  \n20519  Operator: Welcome, everyone, to the Yum! Brand...  \n20520  Operator: Good day. And welcome to the Fourth ...  \n20521  Operator: Welcome to the Fourth Quarter and Fu...  \n20522  Operator: Good morning, ladies and gentlemen, ...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>company name</th>\n      <th>ticker</th>\n      <th>sector</th>\n      <th>industry</th>\n      <th>year</th>\n      <th>quarter</th>\n      <th>date</th>\n      <th>text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>20518</th>\n      <td>ExxonMobil</td>\n      <td>XOM</td>\n      <td>Energy</td>\n      <td>Integrated Oil &amp; Gas</td>\n      <td>2024</td>\n      <td>Q4</td>\n      <td>2025-01-31</td>\n      <td>Jim Chapman: Good morning, everyone. Welcome t...</td>\n    </tr>\n    <tr>\n      <th>20519</th>\n      <td>Yum! Brands</td>\n      <td>YUM</td>\n      <td>Consumer Discretionary</td>\n      <td>Restaurants</td>\n      <td>2024</td>\n      <td>Q4</td>\n      <td>2025-02-06</td>\n      <td>Operator: Welcome, everyone, to the Yum! Brand...</td>\n    </tr>\n    <tr>\n      <th>20520</th>\n      <td>Zebra Technologies</td>\n      <td>ZBRA</td>\n      <td>Information Technology</td>\n      <td>Electronic Equipment &amp; Instruments</td>\n      <td>2024</td>\n      <td>Q4</td>\n      <td>2025-02-13</td>\n      <td>Operator: Good day. And welcome to the Fourth ...</td>\n    </tr>\n    <tr>\n      <th>20521</th>\n      <td>Zoetis</td>\n      <td>ZTS</td>\n      <td>Health Care</td>\n      <td>Pharmaceuticals</td>\n      <td>2024</td>\n      <td>Q4</td>\n      <td>2025-02-13</td>\n      <td>Operator: Welcome to the Fourth Quarter and Fu...</td>\n    </tr>\n    <tr>\n      <th>20522</th>\n      <td>Zimmer Biomet</td>\n      <td>ZBH</td>\n      <td>Health Care</td>\n      <td>Health Care Equipment</td>\n      <td>2024</td>\n      <td>Q4</td>\n      <td>2025-02-06</td>\n      <td>Operator: Good morning, ladies and gentlemen, ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 11,
      "metadata": {
        "gather": {
          "logged": 1745057766072
        }
      },
      "id": "092e145e-c7f9-44ac-a892-c9b322cca190"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the shape\n",
        "df.shape"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 12,
          "data": {
            "text/plain": "(20523, 8)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 12,
      "metadata": {
        "gather": {
          "logged": 1745057766405
        }
      },
      "id": "f4b00977-8552-4da2-852c-690f9ac136e2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the info\n",
        "df.info()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "<class 'pandas.core.frame.DataFrame'>\nRangeIndex: 20523 entries, 0 to 20522\nData columns (total 8 columns):\n #   Column        Non-Null Count  Dtype \n---  ------        --------------  ----- \n 0   company name  20523 non-null  object\n 1   ticker        20523 non-null  object\n 2   sector        20523 non-null  object\n 3   industry      20523 non-null  object\n 4   year          20523 non-null  object\n 5   quarter       20523 non-null  object\n 6   date          20523 non-null  object\n 7   text          20523 non-null  object\ndtypes: object(8)\nmemory usage: 1.3+ MB\n"
        }
      ],
      "execution_count": 13,
      "metadata": {
        "gather": {
          "logged": 1745057766717
        }
      },
      "id": "d4ad9b4b-b101-4257-af4d-a50602375ccd"
    },
    {
      "cell_type": "code",
      "source": [
        "# Check the descriptive statistics\n",
        "df.describe().T"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 14,
          "data": {
            "text/plain": "              count unique                                                top  \\\ncompany name  20523    636                                Danaher Corporation   \nticker        20523    643                                                  A   \nsector        20523     11                                        Industrials   \nindustry      20523    165                              Health Care Equipment   \nyear          20523     11                                               2023   \nquarter       20523      4                                                 Q4   \ndate          20523   2213                                         2023-04-27   \ntext          20523  20360  Executives: Carrie Gillard - Under Armour, Inc...   \n\n              freq  \ncompany name    83  \nticker          44  \nsector        2900  \nindustry       606  \nyear          1976  \nquarter       5158  \ndate            75  \ntext             2  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>count</th>\n      <th>unique</th>\n      <th>top</th>\n      <th>freq</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>company name</th>\n      <td>20523</td>\n      <td>636</td>\n      <td>Danaher Corporation</td>\n      <td>83</td>\n    </tr>\n    <tr>\n      <th>ticker</th>\n      <td>20523</td>\n      <td>643</td>\n      <td>A</td>\n      <td>44</td>\n    </tr>\n    <tr>\n      <th>sector</th>\n      <td>20523</td>\n      <td>11</td>\n      <td>Industrials</td>\n      <td>2900</td>\n    </tr>\n    <tr>\n      <th>industry</th>\n      <td>20523</td>\n      <td>165</td>\n      <td>Health Care Equipment</td>\n      <td>606</td>\n    </tr>\n    <tr>\n      <th>year</th>\n      <td>20523</td>\n      <td>11</td>\n      <td>2023</td>\n      <td>1976</td>\n    </tr>\n    <tr>\n      <th>quarter</th>\n      <td>20523</td>\n      <td>4</td>\n      <td>Q4</td>\n      <td>5158</td>\n    </tr>\n    <tr>\n      <th>date</th>\n      <td>20523</td>\n      <td>2213</td>\n      <td>2023-04-27</td>\n      <td>75</td>\n    </tr>\n    <tr>\n      <th>text</th>\n      <td>20523</td>\n      <td>20360</td>\n      <td>Executives: Carrie Gillard - Under Armour, Inc...</td>\n      <td>2</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 14,
      "metadata": {
        "gather": {
          "logged": 1745057767036
        }
      },
      "id": "42fd7de3-633c-4efb-b641-abf3a5829ba1"
    },
    {
      "cell_type": "code",
      "source": [
        "# print part of an example transcript\n",
        "print(df.iloc[0, ][\"text\"][0:5000])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Executives: Bill Sullivan - President and CEO Ron Nersesian - CEO, Keysight Technologies  Didier Hirsch - SVP, CFO Mike McMullen - President, Chemical Analysis Group Fred Strohmeier - President of Life Sciences and Diagnostics Group Neil Dougherty - CFO, Keysight Guy Séné - SVP of R&D and Sales  Alicia Rodriguez - VP, Investor Relations\nAnalysts: Tycho Peterson - JPMorgan Brandon Couillard - Jefferies Paul Knight - Janney Capital Isaac Ro - Goldman Sachs Ross Muken - ISI Group Tim Evans - Wells Fargo Securities Derik de Bruin - Bank of America Merrill Lynch Jon Groberg - Macquarie Capital Patrick Newton - Stifel Nicolaus Doug Schenkel - Cowen & Company Dan Arias - UBS Bryan Kipp - Janney Capital Markets\nOperator: At this time, I would like to welcome everyone to Q1 ’14 Agilent Technologies Incorporated earnings conference call. [Operator instructions.] Alicia Rodriguez, you may begin your conference. \nAlicia Rodriguez: Thank you, operator, and thank you and welcome everyone to Agilent’s first quarter conference call for fiscal year 2014. With me are Bill Sullivan, Agilent’s President and CEO; Ron Nersesian, CEO of Keysight Technologies; and Didier Hirsch, Agilent Senior Vice President and CFO.  Joining in the Q&A after Didier's comments will be the presidents of our chemical analysis and life sciences and diagnostics groups, Mike McMullen and Fred Strohmeier. Also joining from Keysight will be Neil Dougherty, CFO; and Guy Séné, Senior Vice President of R&D and Sales. You can find the press release and information to supplement today's discussion on our website at www.investor.agilent.com. While there, please click on the link for Financial Results under the Financial Information tab. There you will find an investor presentation along with revenue breakouts, business segment results, and historical financials for Agilent's operations. We will also post a copy of the prepared remarks following this call. Today’s comments by Bill, Ron, and Didier will refer to non-GAAP financial measures. You will find the most directly comparable GAAP financial metrics and reconciliations on our website.  We will make forward-looking statements about the financial performance of the company. These statements are subject to risks and uncertainties and are only valid as of today. The company assumes no obligation to update them. Please look at the company's recent SEC filings for a more complete picture of our risks and other factors. Before turning the call over to Bill, I’d like to remind you that will host its annual analysts meeting in New York City on March 6. Details about the meeting and webcast will be available on the Agilent investor website two weeks prior to that date. And now I’d like to turn the call over to Bill.  \nBill Sullivan: Thanks, Alicia, and hello, everyone. Today Agilent reported first quarter orders of $1.68 billion, down 2% from last year and flat on a core basis. Q1 revenues of $1.68 billion were unchanged from a year ago, up 1% on a core basis. While revenues came in at the low end of guidance, adjusted earnings of $0.67 per share were at the high end of the guidance, up 8% from a year ago. Operating margin was 17.6%.  We saw a mixed business environment with continued steady growth in life science and applied markets. This was offset by continued weakness in our electronic measurement markets, particularly in aerospace and defense. Despite some ongoing economic headwinds, we continue to benefit from our commitment to manage expenses and reduce manufacturing costs.  We also continue to make excellent progress in preparing for the split of the company. On January 7, we announced Keysight Technologies as the name of the new EM company. We expect the separation to be completed by early November. As I indicated last quarter, Agilent will increasingly differentiate our electronic measurement and LDA businesses in preparation for the company’s separation. Today, I will share performance highlights for the life science diagnostics and applied markets. These businesses will be the focus of the new Agilent, as the company continues under my leadership.  Following my remarks, Ron Nersesian will discuss our electronic measurement performance, which will be the focus of the new spinoff company under his leadership. Finally, Didier Hirsch will provide a more detailed discussion of Agilent’s overall financial results as well as our guidance for fiscal second quarter and the full year. Turning to LDA, our first quarter performance continued to show solid revenue growth across instruments, services, and consumables. Q1 revenues of $1 billion increased 5% year over year, reflecting strength across most end markets and a healthy Q4 backlog. Q1 orders of $979 million increased 2% over last year. The slowing in the order growth is driven by weaker demand in academic and government markets. Operating margins were up 210 basis points to 19.2%, consistent with our margin expansion goals for the businesses. We continue to f\n"
        }
      ],
      "execution_count": 15,
      "metadata": {
        "gather": {
          "logged": 1745057767481
        }
      },
      "id": "5a3e9218-0b1d-4230-8021-20ef2312c244"
    },
    {
      "cell_type": "code",
      "source": [
        "# sector distribution\n",
        "df[\"sector\"].value_counts()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 16,
          "data": {
            "text/plain": "sector\nIndustrials               2900\nFinancials                2883\nHealth Care               2566\nInformation Technology    2513\nConsumer Discretionary    2394\nConsumer Staples          1555\nReal Estate               1262\nUtilities                 1172\nMaterials                 1151\nEnergy                    1111\nCommunication Services    1016\nName: count, dtype: int64"
          },
          "metadata": {}
        }
      ],
      "execution_count": 16,
      "metadata": {
        "gather": {
          "logged": 1745057767794
        }
      },
      "id": "79d0ced8-9cd7-46f3-aa1b-a8bae386fdc7"
    },
    {
      "cell_type": "code",
      "source": [
        "# year distribution\n",
        "df[\"year\"].value_counts().sort_index()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 17,
          "data": {
            "text/plain": "year\n2014    1622\n2015    1679\n2016    1760\n2017    1820\n2018    1870\n2019    1934\n2020    1964\n2021    1967\n2022    1973\n2023    1976\n2024    1958\nName: count, dtype: int64"
          },
          "metadata": {}
        }
      ],
      "execution_count": 17,
      "metadata": {
        "gather": {
          "logged": 1745057768117
        }
      },
      "id": "85a94af4-7519-4b2b-bb68-a898b0260a60"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Optionally: Choose a random sample"
      ],
      "metadata": {},
      "id": "93072147-c40b-409f-96da-801bc1f1c2b8"
    },
    {
      "cell_type": "code",
      "source": [
        "# # uncomment for the final analysis\n",
        "# # select a sample for development purposes\n",
        "# sample_size = 2000\n",
        "# df = df.sample(sample_size, random_state=seed)\n",
        "\n",
        "# # check the shape\n",
        "# df.shape"
      ],
      "outputs": [],
      "execution_count": 18,
      "metadata": {
        "gather": {
          "logged": 1745057768412
        }
      },
      "id": "3ddf40e6-ab1b-43c8-8da3-65667760a28a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Topic modelling: Fundamentals"
      ],
      "metadata": {},
      "id": "f1f09556-106f-4d1a-b8a0-b93f924cf27c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Preprocessing includes (1) cleaning (with sentence- and word-level tokenization) and (2) feature extraction (creating a numerical representation of the text, i.e. a document-term matrix or DTM for short). NMF and LDA require both but differ in the best feature engineering; BERTopic internalizes feature engineering (no DTM as input needed) and can handle uncleaned and cleaned text."
      ],
      "metadata": {},
      "id": "e0d7a4b0-e10a-4451-9acd-1bae64ff17ab"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing 1: Cleaning"
      ],
      "metadata": {},
      "id": "d8550649-9422-42cb-8f33-c9a22d64f79f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Replace financial numbers with the word \"fin_num\""
      ],
      "metadata": {},
      "id": "5d047e2f-78ee-415a-aed8-78b80e0ca417"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Financial numbers are replaced with \"fin_num\" to normalize numeric expressions, reduce noise, and retain meaningful signals for modeling."
      ],
      "metadata": {},
      "id": "67158e32-63cc-4b3c-adcd-b60b94aa1812"
    },
    {
      "cell_type": "code",
      "source": [
        "# define function\n",
        "def substitute_financial_numbers(string):\n",
        "    '''\n",
        "    Substitues financial numbers by \"fin_num\" in a string\n",
        "    '''\n",
        "    sub_string = re.sub(\n",
        "        r\"\\$\\s?[0-9.,']+(?:\\s?(?:million|billion|thousand))?|[0-9.,']+%\",\n",
        "        \" fin_num \", \n",
        "        string) \n",
        "    return sub_string"
      ],
      "outputs": [],
      "execution_count": 19,
      "metadata": {
        "gather": {
          "logged": 1745057768686
        }
      },
      "id": "e9df6921-a03e-4ea0-9464-78299a7d01f9"
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply function to df\n",
        "df[\"text_clean\"] = df[\"text\"].apply(substitute_financial_numbers)"
      ],
      "outputs": [],
      "execution_count": 20,
      "metadata": {
        "gather": {
          "logged": 1745057784578
        }
      },
      "id": "7ff9250d-ee31-4775-8bb1-ed54f7e38fae"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Filter words and sentences"
      ],
      "metadata": {},
      "id": "8787d28a-0ae2-47e4-b3f0-70a480fbb22e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: We require words to contain only alphabetic characters, be at least three characters long, and to be not first names (F); exceptions are \"AI\", \"US\", \"fin_num\". In addition, we require sentences to contain fewer than 50% occurrences of the word \"fin_num\" and to be at least five words long. (F) Footnote: First results showed that the DTM and the resulting topic-word-matrices contain many first names; filtering by NER is too slow and only somewhat effective; therefore, we choose a simpler and more effective approach here.  "
      ],
      "metadata": {},
      "id": "676ea1b1-319c-40b3-ba40-fec12363b693"
    },
    {
      "cell_type": "code",
      "source": [
        "# define function\n",
        "def filter_words(string, common_names, word_length=3, exceptions=None):\n",
        "    '''\n",
        "    Filters words in a string\n",
        "    - Req. 1: word is alpha (excl. numbers and special characters)\n",
        "    - Req. 2: word has a certain length (default: 3 characters) \n",
        "    - Req. 3: word is not a first name\n",
        "    - Exceptions: list of words exempt from the req (e.g., \"AI\") (default: None)\n",
        "    '''\n",
        "    if exceptions is None:\n",
        "        exceptions = []\n",
        "    keep_words = []\n",
        "    for word in word_tokenize(string):\n",
        "        if ((word.isalpha() \n",
        "             and len(word) >= word_length \n",
        "             and word.lower() not in common_names) \n",
        "            or word in exceptions):\n",
        "            keep_words.append(word)\n",
        "    return \" \".join(keep_words)\n",
        "\n",
        "# test the function\n",
        "test_string = '''Artificial intelligence, or short AI, boosts return \n",
        "fin_num by a factor of 10. However, Jim not 100%. Revenue fin_num fin_num fin_num.'''\n",
        "\n",
        "print(\"Test:\", filter_words(test_string, common_names=[\"Jim\"], exceptions=[\"AI\", \"fin_num\"]))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Test: Artificial intelligence short AI boosts return fin_num factor However Jim not Revenue fin_num fin_num fin_num\n"
        }
      ],
      "execution_count": 21,
      "metadata": {
        "gather": {
          "logged": 1745057784881
        }
      },
      "id": "8106e8dd-a467-4c8f-bc0a-65d41c25a734"
    },
    {
      "cell_type": "code",
      "source": [
        "# define function\n",
        "def filter_words_sentences(string, common_names, word_length=3, exceptions=None, \n",
        "                           sent_length=5, fin_num_th=0.5):\n",
        "    '''\n",
        "    Filters words (by call of filter_words) and sentences in a string\n",
        "    - Req. 1, 2, 3 and exceptions: see function filter_words\n",
        "    - Req. 4: sentence has certain length (default: 5 words)\n",
        "    - Req. 5: sentence has less than certain percentage of word \"fin_num\" \n",
        "      in it default: 0.5)\n",
        "    '''\n",
        "    keep_sents = []\n",
        "    for sent in sent_tokenize(string):\n",
        "        words = filter_words(sent, common_names, word_length, exceptions).split() # incl. word_tokenize\n",
        "        if (len(words) >= sent_length \n",
        "            and words.count(\"fin_num\")/len(words) <= fin_num_th):\n",
        "            keep_sents.append(\" \".join(words))\n",
        "    return \" \".join(keep_sents)\n",
        "\n",
        "# test the function:\n",
        "print(\"Test:\", filter_words_sentences(test_string, common_names=[\"Jim\"], exceptions=[\"AI\", \"fin_num\"]))"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Test: Artificial intelligence short AI boosts return fin_num factor\n"
        }
      ],
      "execution_count": 22,
      "metadata": {
        "gather": {
          "logged": 1745057785124
        }
      },
      "id": "30a109c4-ed4f-48fb-a92d-689b75122e87"
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply function to df\n",
        "\n",
        "# define common names (US)\n",
        "number_names = 1000\n",
        "nd = NameDataset()\n",
        "top_male = nd.get_top_names(n=number_names, gender='Male', country_alpha2='US')['US']['M']\n",
        "top_female = nd.get_top_names(n=number_names, gender='Female', country_alpha2='US')['US']['F']\n",
        "common_names = set(name.lower() for name in top_male + top_female)\n",
        "\n",
        "# define exceptions\n",
        "exceptions = [\"AI\", \"US\", \"fin_num\"]\n",
        "\n",
        "# call function\n",
        "df[\"text_clean\"] = df[\"text\"].apply(\n",
        "    filter_words_sentences,\n",
        "    word_length=3,\n",
        "    common_names=common_names,\n",
        "    exceptions=exceptions, \n",
        "    sent_length=5, \n",
        "    fin_num_th=0.5)"
      ],
      "outputs": [],
      "execution_count": 23,
      "metadata": {
        "gather": {
          "logged": 1745059124448
        }
      },
      "id": "b823bd6f-5fc9-4e03-b2c7-f148a10ef485"
    },
    {
      "cell_type": "code",
      "source": [
        "# View first entries\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 24,
          "data": {
            "text/plain": "             company name ticker                  sector  \\\n0    Agilent Technologies      A             Health Care   \n1              Apple Inc.   AAPL  Information Technology   \n2                  AbbVie   ABBV             Health Care   \n3  AmerisourceBergen Corp    ABC             Health Care   \n4     Abbott Laboratories    ABT             Health Care   \n\n                                     industry  year quarter        date  \\\n0              Life Sciences Tools & Services  2014      Q1  2014-02-13   \n1  Technology Hardware, Storage & Peripherals  2014      Q1  2014-01-27   \n2                               Biotechnology  2014      Q1  2014-04-25   \n3                    Health Care Distributors  2014      Q1  2014-04-24   \n4                       Health Care Equipment  2014      Q1  2014-04-16   \n\n                                                text  \\\n0  Executives: Bill Sullivan - President and CEO ...   \n1  Executives: Tim Cook - CEO Peter Oppenheimer -...   \n2  Executives: Richard Gonzalez – Chairman, Chief...   \n3  Operator: Greetings, and welcome to the CoreSi...   \n4  Operator: Good morning and thank you for stand...   \n\n                                          text_clean  \n0  Executives Sullivan President and CEO Nersesia...  \n1  Executives Cook CEO Oppenheimer SVP CFO Luca M...  \n2  Executives Chairman Chief Executive Officer Ex...  \n3  Operator Greetings and welcome the CoreSite Re...  \n4  Operator Good morning and thank you for standi...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>company name</th>\n      <th>ticker</th>\n      <th>sector</th>\n      <th>industry</th>\n      <th>year</th>\n      <th>quarter</th>\n      <th>date</th>\n      <th>text</th>\n      <th>text_clean</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Agilent Technologies</td>\n      <td>A</td>\n      <td>Health Care</td>\n      <td>Life Sciences Tools &amp; Services</td>\n      <td>2014</td>\n      <td>Q1</td>\n      <td>2014-02-13</td>\n      <td>Executives: Bill Sullivan - President and CEO ...</td>\n      <td>Executives Sullivan President and CEO Nersesia...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Apple Inc.</td>\n      <td>AAPL</td>\n      <td>Information Technology</td>\n      <td>Technology Hardware, Storage &amp; Peripherals</td>\n      <td>2014</td>\n      <td>Q1</td>\n      <td>2014-01-27</td>\n      <td>Executives: Tim Cook - CEO Peter Oppenheimer -...</td>\n      <td>Executives Cook CEO Oppenheimer SVP CFO Luca M...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AbbVie</td>\n      <td>ABBV</td>\n      <td>Health Care</td>\n      <td>Biotechnology</td>\n      <td>2014</td>\n      <td>Q1</td>\n      <td>2014-04-25</td>\n      <td>Executives: Richard Gonzalez – Chairman, Chief...</td>\n      <td>Executives Chairman Chief Executive Officer Ex...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AmerisourceBergen Corp</td>\n      <td>ABC</td>\n      <td>Health Care</td>\n      <td>Health Care Distributors</td>\n      <td>2014</td>\n      <td>Q1</td>\n      <td>2014-04-24</td>\n      <td>Operator: Greetings, and welcome to the CoreSi...</td>\n      <td>Operator Greetings and welcome the CoreSite Re...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Abbott Laboratories</td>\n      <td>ABT</td>\n      <td>Health Care</td>\n      <td>Health Care Equipment</td>\n      <td>2014</td>\n      <td>Q1</td>\n      <td>2014-04-16</td>\n      <td>Operator: Good morning and thank you for stand...</td>\n      <td>Operator Good morning and thank you for standi...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 24,
      "metadata": {
        "gather": {
          "logged": 1745059126879
        }
      },
      "id": "8e7d226a-07f7-4564-b49c-9be4f01212bb"
    },
    {
      "cell_type": "code",
      "source": [
        "# print part of a cleaned example transcript\n",
        "print(df.iloc[0, ][\"text_clean\"][0:5000])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "Executives Sullivan President and CEO Nersesian CEO Keysight Technologies Didier Hirsch SVP CFO McMullen President Chemical Analysis Group Strohmeier President Life Sciences and Diagnostics Group Dougherty CFO Keysight Séné SVP and Sales Investor Relations Analysts Tycho Peterson JPMorgan Couillard Jefferies Knight Janney Capital Goldman Sachs Muken ISI Group Evans Wells Fargo Securities Derik Bruin Bank Merrill Lynch Groberg Macquarie Capital Newton Stifel Nicolaus Schenkel Cowen Company Arias UBS Kipp Janney Capital Markets Operator this time would like welcome everyone Agilent Technologies Incorporated earnings conference call Thank you operator and thank you and welcome everyone Agilent first quarter conference call for fiscal year With are Sullivan Agilent President and CEO Nersesian CEO Keysight Technologies and Didier Hirsch Agilent Senior Vice President and CFO Joining the after Didier comments the presidents our chemical analysis and life sciences and diagnostics groups McMullen and Strohmeier Also joining from Keysight Dougherty CFO and Séné Senior Vice President and Sales You can find the press release and information supplement today discussion our website While there please click the link for Financial Results under the Financial Information tab There you find investor presentation along with revenue breakouts business segment results and historical financials for Agilent operations also post copy the prepared remarks following this call Today comments and Didier refer financial measures You find the most directly comparable GAAP financial metrics and reconciliations our website make statements about the financial performance the company These statements are subject risks and uncertainties and are only valid today The company assumes obligation update them Please look the company recent SEC filings for more complete picture our risks and other factors Before turning the call over like remind you that host its annual analysts meeting New York City March Details about the meeting and webcast available the Agilent investor website two weeks prior that date And now like turn the call over Sullivan Thanks and hello everyone Today Agilent reported first quarter orders billion down from last year and flat core basis revenues billion were unchanged from year ago core basis While revenues came the low end guidance adjusted earnings per share were the high end the guidance from year ago saw mixed business environment with continued steady growth life science and applied markets This was offset continued weakness our electronic measurement markets particularly aerospace and defense Despite some ongoing economic headwinds continue benefit from our commitment manage expenses and reduce manufacturing costs also continue make excellent progress preparing for the split the company January announced Keysight Technologies the name the new company expect the separation completed early November indicated last quarter Agilent increasingly differentiate our electronic measurement and LDA businesses preparation for the company separation Today share performance highlights for the life science diagnostics and applied markets These businesses the focus the new Agilent the company continues under leadership Following remarks Nersesian discuss our electronic measurement performance which the focus the new spinoff company under his leadership Finally Didier Hirsch provide more detailed discussion Agilent overall financial results well our guidance for fiscal second quarter and the full year Turning LDA our first quarter performance continued show solid revenue growth across instruments services and consumables revenues billion increased year over year reflecting strength across most end markets and healthy backlog orders million increased over last year The slowing the order growth driven weaker demand academic and government markets Operating margins were basis points consistent with our margin expansion goals for the businesses continue focus attractive end markets our leading product portfolio and significant operational leverage Our end market performance LDA was particularly strong pharmaceutical biotech clinical food and forensics Pharma revenue grew year over year with strength Europe and Japan offsetting slow demand the Food revenues were over last year globalization the industry continued drive demand for food safety Forensics grew driven the need identify and characterize new designer drugs entering the global market And energy was led Europe and large refinery projects the Middle East Conversely academic and government markets declined year over year Research spending remains constrained impacted slow budget releases particularly the and China Diagnostics and clinical revenues were The unintelligible was slow due very slow start for the quarter but the clinical business was robust driven CGH arrays and target enrichment regional basis LDA performance was mixed Europe continued see the strongest regional perform\n"
        }
      ],
      "execution_count": 25,
      "metadata": {
        "gather": {
          "logged": 1745059127705
        }
      },
      "id": "2997b2da-abb0-43a4-9d2c-10ecde2570aa"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Lemmatization"
      ],
      "metadata": {},
      "id": "2e252070-4910-4668-90bb-13264bd278d5"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: We lemmatize words. Lemmatizing converts words to their base forms, reducing the inflectional variability in your texts. This is only hedlpful for LDA and NMF, as BERTopic can deal with this. Footnote: Earlier attempts showed that diffreent forms of the same word (e.g., singular and plural) showed up in the topic."
      ],
      "metadata": {},
      "id": "77104d42-f0d2-47f8-884a-300fd5f9ccd4"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function: \n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "def lemmatize_text(string):\n",
        "    \"\"\"\n",
        "    Lemmatizes the string\n",
        "    \"\"\"\n",
        "    doc = nlp(string)\n",
        "    return \" \".join(token.lemma_ for token in doc)"
      ],
      "outputs": [],
      "execution_count": 26,
      "metadata": {
        "gather": {
          "logged": 1745059129911
        }
      },
      "id": "13fb4def-a5db-4b59-8574-8fdda5a4fc6b"
    },
    {
      "cell_type": "code",
      "source": [
        "# Appply the function\n",
        "tqdm.pandas()\n",
        "df[\"text_clean_lemma\"] = df[\"text_clean\"].progress_apply(lemmatize_text)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": "100%|██████████| 20523/20523 [3:54:55<00:00,  1.46it/s]  \n"
        }
      ],
      "execution_count": 27,
      "metadata": {
        "gather": {
          "logged": 1745073234142
        }
      },
      "id": "96c1d30c-1373-48ca-bd98-be7af7c5a28d"
    },
    {
      "cell_type": "code",
      "source": [
        "# View first entries\n",
        "df.head()"
      ],
      "outputs": [
        {
          "output_type": "execute_result",
          "execution_count": 28,
          "data": {
            "text/plain": "             company name ticker                  sector  \\\n0    Agilent Technologies      A             Health Care   \n1              Apple Inc.   AAPL  Information Technology   \n2                  AbbVie   ABBV             Health Care   \n3  AmerisourceBergen Corp    ABC             Health Care   \n4     Abbott Laboratories    ABT             Health Care   \n\n                                     industry  year quarter        date  \\\n0              Life Sciences Tools & Services  2014      Q1  2014-02-13   \n1  Technology Hardware, Storage & Peripherals  2014      Q1  2014-01-27   \n2                               Biotechnology  2014      Q1  2014-04-25   \n3                    Health Care Distributors  2014      Q1  2014-04-24   \n4                       Health Care Equipment  2014      Q1  2014-04-16   \n\n                                                text  \\\n0  Executives: Bill Sullivan - President and CEO ...   \n1  Executives: Tim Cook - CEO Peter Oppenheimer -...   \n2  Executives: Richard Gonzalez – Chairman, Chief...   \n3  Operator: Greetings, and welcome to the CoreSi...   \n4  Operator: Good morning and thank you for stand...   \n\n                                          text_clean  \\\n0  Executives Sullivan President and CEO Nersesia...   \n1  Executives Cook CEO Oppenheimer SVP CFO Luca M...   \n2  Executives Chairman Chief Executive Officer Ex...   \n3  Operator Greetings and welcome the CoreSite Re...   \n4  Operator Good morning and thank you for standi...   \n\n                                    text_clean_lemma  \n0  executive Sullivan President and CEO Nersesian...  \n1  executive Cook CEO Oppenheimer SVP CFO Luca Ma...  \n2  executive Chairman Chief Executive Officer Exe...  \n3  Operator greeting and welcome the CoreSite Rea...  \n4  operator good morning and thank you for stand ...  ",
            "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>company name</th>\n      <th>ticker</th>\n      <th>sector</th>\n      <th>industry</th>\n      <th>year</th>\n      <th>quarter</th>\n      <th>date</th>\n      <th>text</th>\n      <th>text_clean</th>\n      <th>text_clean_lemma</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>Agilent Technologies</td>\n      <td>A</td>\n      <td>Health Care</td>\n      <td>Life Sciences Tools &amp; Services</td>\n      <td>2014</td>\n      <td>Q1</td>\n      <td>2014-02-13</td>\n      <td>Executives: Bill Sullivan - President and CEO ...</td>\n      <td>Executives Sullivan President and CEO Nersesia...</td>\n      <td>executive Sullivan President and CEO Nersesian...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>Apple Inc.</td>\n      <td>AAPL</td>\n      <td>Information Technology</td>\n      <td>Technology Hardware, Storage &amp; Peripherals</td>\n      <td>2014</td>\n      <td>Q1</td>\n      <td>2014-01-27</td>\n      <td>Executives: Tim Cook - CEO Peter Oppenheimer -...</td>\n      <td>Executives Cook CEO Oppenheimer SVP CFO Luca M...</td>\n      <td>executive Cook CEO Oppenheimer SVP CFO Luca Ma...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>AbbVie</td>\n      <td>ABBV</td>\n      <td>Health Care</td>\n      <td>Biotechnology</td>\n      <td>2014</td>\n      <td>Q1</td>\n      <td>2014-04-25</td>\n      <td>Executives: Richard Gonzalez – Chairman, Chief...</td>\n      <td>Executives Chairman Chief Executive Officer Ex...</td>\n      <td>executive Chairman Chief Executive Officer Exe...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>AmerisourceBergen Corp</td>\n      <td>ABC</td>\n      <td>Health Care</td>\n      <td>Health Care Distributors</td>\n      <td>2014</td>\n      <td>Q1</td>\n      <td>2014-04-24</td>\n      <td>Operator: Greetings, and welcome to the CoreSi...</td>\n      <td>Operator Greetings and welcome the CoreSite Re...</td>\n      <td>Operator greeting and welcome the CoreSite Rea...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>Abbott Laboratories</td>\n      <td>ABT</td>\n      <td>Health Care</td>\n      <td>Health Care Equipment</td>\n      <td>2014</td>\n      <td>Q1</td>\n      <td>2014-04-16</td>\n      <td>Operator: Good morning and thank you for stand...</td>\n      <td>Operator Good morning and thank you for standi...</td>\n      <td>operator good morning and thank you for stand ...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
          },
          "metadata": {}
        }
      ],
      "execution_count": 28,
      "metadata": {
        "gather": {
          "logged": 1745073236613
        }
      },
      "id": "8c36ba27-8ef5-46ae-a911-cbc1665cbf9e"
    },
    {
      "cell_type": "code",
      "source": [
        "# print part of a cleaned example transcript\n",
        "print(df.iloc[0, ][\"text_clean_lemma\"][0:5000])"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "executive Sullivan President and CEO Nersesian CEO Keysight Technologies Didier Hirsch SVP CFO McMullen President Chemical Analysis Group Strohmeier President Life Sciences and Diagnostics Group Dougherty CFO Keysight Séné SVP and Sales Investor Relations Analysts Tycho Peterson JPMorgan Couillard Jefferies Knight Janney Capital Goldman Sachs Muken ISI Group Evans Wells Fargo Securities Derik Bruin Bank Merrill Lynch Groberg Macquarie Capital Newton Stifel Nicolaus Schenkel Cowen Company Arias UBS Kipp Janney Capital Markets Operator this time would like welcome everyone Agilent Technologies incorporated earning conference call thank you operator and thank you and welcome everyone Agilent first quarter conference call for fiscal year with be Sullivan Agilent President and CEO Nersesian CEO Keysight Technologies and Didier Hirsch Agilent Senior Vice President and CFO join the after Didier comment the president our chemical analysis and life science and diagnostic group McMullen and Strohmeier also join from Keysight Dougherty CFO and Séné Senior Vice President and sale you can find the press release and information supplement today discussion our website while there please click the link for Financial result under the Financial Information tab there you find investor presentation along with revenue breakout business segment result and historical financial for Agilent operation also post copy the prepared remark follow this call today comment and Didier refer financial measure you find the most directly comparable GAAP financial metric and reconciliation our website make statement about the financial performance the company these statement be subject risk and uncertainty and be only valid today the company assume obligation update they please look the company recent SEC filing for more complete picture our risk and other factor before turn the call over like remind you that host its annual analyst meet New York City March Details about the meeting and webcast available the Agilent investor website two week prior that date and now like turn the call over Sullivan Thanks and hello everyone Today Agilent report first quarter order billion down from last year and flat core basis revenue billion be unchanged from year ago core basis while revenue come the low end guidance adjust earning per share be the high end the guidance from year ago see mixed business environment with continue steady growth life science and apply market this be offset continued weakness our electronic measurement market particularly aerospace and defense despite some ongoing economic headwind continue benefit from our commitment manage expense and reduce manufacturing cost also continue make excellent progress prepare for the split the company January announce Keysight Technologies the name the new company expect the separation complete early November indicate last quarter Agilent increasingly differentiate our electronic measurement and LDA business preparation for the company separation today share performance highlight for the life science diagnostic and apply market these business the focus the new Agilent the company continue under leadership follow remark Nersesian discuss our electronic measurement performance which the focus the new spinoff company under his leadership Finally Didier Hirsch provide more detailed discussion Agilent overall financial result well our guidance for fiscal second quarter and the full year turn LDA our first quarter performance continue show solid revenue growth across instrument service and consumable revenue billion increase year over year reflect strength across most end market and healthy backlog order million increase over last year the slow the order growth drive weak demand academic and government market operating margin be basis point consistent with our margin expansion goal for the business continue focus attractive end market our lead product portfolio and significant operational leverage our end market performance LDA be particularly strong pharmaceutical biotech clinical food and forensic Pharma revenue grow year over year with strength Europe and Japan offset slow demand the Food revenue be over last year globalization the industry continue drive demand for food safety forensic grew drive the need identify and characterize new designer drug enter the global market and energy be lead Europe and large refinery project the Middle East conversely academic and government market decline year over year Research spending remain constrain impact slow budget release particularly the and China Diagnostics and clinical revenue be the unintelligible be slow due very slow start for the quarter but the clinical business be robust drive CGH array and target enrichment regional basis LDA performance be mixed Europe continue see the strong regional performance drive strength pharma and service also show strong growth while Japan be down primarily due weak currency Americas be slightly constrain delay budget rel\n"
        }
      ],
      "execution_count": 29,
      "metadata": {
        "gather": {
          "logged": 1745073237450
        }
      },
      "id": "9bfa0b44-d609-49bb-a45b-49e6de31b3db"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Optionally: Save or load preprocessed df"
      ],
      "metadata": {},
      "id": "c0b5cff6-3def-46ef-8ca2-2b1f6a837433"
    },
    {
      "cell_type": "code",
      "source": [
        "# To save:\n",
        "df.to_csv(\"./df.csv.gz\",index=False)"
      ],
      "outputs": [],
      "execution_count": 30,
      "metadata": {
        "gather": {
          "logged": 1745073487132
        }
      },
      "id": "77c79044-27cf-4332-b51a-50dafd70cf28"
    },
    {
      "cell_type": "code",
      "source": [
        "# # To load:\n",
        "# df = pd.read_csv(\"./df.csv.gz\")\n",
        "\n",
        "# # Check for NaNs introduced through saving: \n",
        "# problem_rows = df[~df[\"text_clean_lemma\"].apply(lambda x: isinstance(x, str))]\n",
        "# print(f\"Problem rows: {len(problem_rows)}\")\n",
        "# print(problem_rows.head())\n",
        "\n",
        "# # Drop NaN rows intriduced through saving:\n",
        "# df = df[df[\"text_clean_lemma\"].notna()]"
      ],
      "outputs": [],
      "execution_count": 31,
      "metadata": {
        "gather": {
          "logged": 1745073487867
        }
      },
      "id": "46e56958-3368-4901-955e-9de1c37bc6a7"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preprocessing 2: Feature extraction/ Document-term matrix (DTM) "
      ],
      "metadata": {},
      "id": "bae806a9-5809-494c-b6cc-8e21c22eab23"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: There are two options for DTMs: (1) DTM-TF, which includes simple word counts (term frequencies) for each document, and (2) DTM-TF-IDF, which weights terms by their inverse frequency in the corpus (term frequency–inverse document frequency). LDA requires DTM-TF, while NMF works best with DTM-TF-IDF. (BERTopic requires neither.) For both DTMs, we are case-insensitive (convert all text to lowercase), consider unigrams (single words) and bigrams (expressions consisting of two words), remove stopwords (i.e., common words that typically do not influence meaning), exclude words that appear in more than 50% of documents, and restrict the vocabulary to the 1,000 most frequent words. "
      ],
      "metadata": {},
      "id": "2db520e8-dd0e-4a14-b5f2-0258a2a42b11"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DTM-TF"
      ],
      "metadata": {},
      "id": "00c597aa-07c4-4fdb-843e-1cab7989db6e"
    },
    {
      "cell_type": "code",
      "source": [
        "# load stop words\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Create DTM-TF\n",
        "start = time.time()\n",
        "\n",
        "vec_tf = CountVectorizer(\n",
        "    analyzer=\"word\", # Tokenize text at the word level\n",
        "    tokenizer=lambda x: x.split(), # simple split bc text pre-tokenized (see preprocess.)\n",
        "    token_pattern=None, # disable regex-based tokeniz. bc text pre-tokenized (see preprocess.)\n",
        "    lowercase=True, # convert tokens to lowercase\n",
        "    stop_words=list(stops), # exclude stop words\n",
        "    ngram_range=(1, 2), # allow unigrams and bigrams\n",
        "    max_df=0.5, # exclude tokens appearing in >50% of docs\n",
        "    max_features=1000) # limit vocabulart to 1000 most common tokens\n",
        "\n",
        "dtm_tf = vec_tf.fit_transform(df[\"text_clean_lemma\"])\n",
        "vocab_tf = vec_tf.get_feature_names_out()\n",
        "\n",
        "end = time.time()\n",
        "print(f\"time elapsed (seconds): {end - start}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "time elapsed (seconds): 182.61599397659302\n"
        }
      ],
      "execution_count": 32,
      "metadata": {
        "gather": {
          "logged": 1745073669651
        }
      },
      "id": "429449fc-686f-4f28-8c51-82c37c972125"
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check:\n",
        "display(dtm_tf.todense())\n",
        "dtm_tf.shape"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "matrix([[ 1,  2,  0, ..., 12,  1,  1],\n        [ 0,  0,  1, ..., 22,  0,  0],\n        [ 1,  0,  0, ...,  1,  0,  0],\n        ...,\n        [ 2,  2,  0, ...,  0,  0,  0],\n        [ 0,  2,  1, ...,  0,  0,  0],\n        [ 0,  5,  0, ...,  1,  0,  1]])"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 33,
          "data": {
            "text/plain": "(20523, 1000)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 33,
      "metadata": {
        "gather": {
          "logged": 1745073672162
        }
      },
      "id": "953b0522-3454-4e24-8d62-b6db0bb5cb76"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### DTM-TF-IDF"
      ],
      "metadata": {},
      "id": "bb164cfb-658a-4a10-be3a-31cbb0f1c5ee"
    },
    {
      "cell_type": "code",
      "source": [
        "# load stop words\n",
        "stops = set(stopwords.words(\"english\"))\n",
        "\n",
        "# Create DTM-TF\n",
        "start = time.time()\n",
        "\n",
        "vec_tfidf = TfidfVectorizer(\n",
        "    analyzer=\"word\", # Tokenize text at the word level\n",
        "    tokenizer=lambda x: x.split(), # simple split bc text pre-tokenized (see preprocess.)\n",
        "    token_pattern=None, # disable regex-based tokeniz. bc text pre-tokenized (see preprocess.)\n",
        "    lowercase=True, # convert tokens to lowercase\n",
        "    stop_words=list(stops), # exclude stop words\n",
        "    ngram_range=(1, 2), # allow unigrams and bigrams\n",
        "    max_df=0.5, # exclude tokens appearing in >50% of docs\n",
        "    max_features=1000) # limit vocabulart to 1000 most common tokens\n",
        "\n",
        "dtm_tfidf = vec_tfidf.fit_transform(df[\"text_clean_lemma\"])\n",
        "vocab_tfidf = vec_tfidf.get_feature_names_out()\n",
        "\n",
        "end = time.time()\n",
        "print(f\"time elapsed (seconds): {end - start}\")"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "time elapsed (seconds): 187.48503375053406\n"
        }
      ],
      "execution_count": 34,
      "metadata": {
        "gather": {
          "logged": 1745073857193
        }
      },
      "id": "5d4e92fc-31b0-4374-9844-d8ad2129945f"
    },
    {
      "cell_type": "code",
      "source": [
        "# sanity check:\n",
        "display(dtm_tfidf.todense())\n",
        "dtm_tfidf.shape"
      ],
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "matrix([[0.01442843, 0.02887781, 0.        , ..., 0.17916472, 0.01370502,\n         0.01839409],\n        [0.        , 0.        , 0.01282105, ..., 0.26917476, 0.        ,\n         0.        ],\n        [0.00834295, 0.        , 0.        , ..., 0.00863319, 0.        ,\n         0.        ],\n        ...,\n        [0.0193613 , 0.01937535, 0.        , ..., 0.        , 0.        ,\n         0.        ],\n        [0.        , 0.02146574, 0.0116296 , ..., 0.        , 0.        ,\n         0.        ],\n        [0.        , 0.08084033, 0.        , ..., 0.01671841, 0.        ,\n         0.02059692]])"
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "execution_count": 35,
          "data": {
            "text/plain": "(20523, 1000)"
          },
          "metadata": {}
        }
      ],
      "execution_count": 35,
      "metadata": {
        "gather": {
          "logged": 1745073859298
        }
      },
      "id": "68cb4f69-a880-4079-b388-0e396da3f177"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## LDA, NMF and BERTopic models"
      ],
      "metadata": {},
      "id": "995d888c-a0ab-4c95-b1b2-700e50be901f"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Theoretical background "
      ],
      "metadata": {},
      "id": "746ab8e0-fb29-4a9c-92b6-f9b41a469e40"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LDA notes"
      ],
      "metadata": {},
      "id": "07942d0a-c1df-4e86-bf85-6ef449a2de9e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use topic_word_prior = 0.01, doc_topic_prior = 50/n; these priors are not the default but often recommended in the literature; the model is stochastic"
      ],
      "metadata": {},
      "id": "620541f7-4b98-43ed-9d34-a46db80d7c19"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation of output:\n",
        "- Document-topic matrix: Each row represents a document (e.g., a transcript of an earnings call), and each column represents a topic. The matrix can be interpreted as a topic distribution for each document, i.e., the proportion or probability that a document is associated with a given topic (for each row: sum over columns = 1).\n",
        "- Topic-word matrix: Each row corresponds to a topic, and each column corresponds to a word from the vocabulary in the DTM. Originally, the values (from model.components_) contain pseudocounts, i.e. estimated number of times word j assigned to topic i. After normalization (m_topic_word = m_doc_topic / m_doc_topic(axis=1, keepdims=True), the can be viewed as word distribution for each topic, i.e., the probability that a given word appears in a topic (for each row: sum over columns = 1)."
      ],
      "metadata": {},
      "id": "935f4a0a-ea86-4e16-9ac6-e861f487b192"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NMF notes"
      ],
      "metadata": {},
      "id": "b073131c-755f-4665-840f-dcf50581fdf4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "The model is deterministic."
      ],
      "metadata": {},
      "id": "18f80a6c-6867-433e-b7d1-fe23aa1f2172"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretiation of the output: \n",
        "- Document-topic matrix: Each row represents a document (transcript of an earnings call), and each column represents a topic. Originally, the values (from model.fit_transform) reflect the strength or weight of each topic in the corresponding document — that is, how much the document loads onto each topic. After normalization (m_doc_topic / m_doc_topic.sum(axis=1, keepdims=True)), the matrix can be interpreted as a topic distribution for each document, i.e., the proportion or probability that a document is associated with a given topic (for each row: sum over columns = 1).\n",
        "\n",
        "- Topic-word matrix: Each row corresponds to a topic, and each column corresponds to a word from the vocabulary in the DTM. Originally, the values (from model.components_) represent the strength or weight of association between word j and topic i. After normalization (model.components_ / model.components_.sum(axis=1, keepdims=True)), the matrix can be interpreted as a word distribution for each topic, i.e., the probability that a given word appears in a topic (for each row: sum over columns = 1).\n"
      ],
      "metadata": {},
      "id": "3f09e47e-6f62-42ac-aeb6-0d01ab9a4a36"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BERTOpic notes"
      ],
      "metadata": {},
      "id": "b67b724e-1c3e-4e6e-8985-f3659dd53521"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Default BERTopic incolves the following steps (source: GPT):\n",
        "\n",
        "1. Embedding Documents – Using a transformer model on each document to convert its raw text into dense vector representations that capture the semantic meaning \n",
        "2. Reducing Dimensionality – Use UMAP to compress the high-dimensional embeddings into a lower-dimensional space while preserving their structural relationships\n",
        "3. Clustering Reduced Embeddings into Topics – Apply HDBSCAN to group similar document embeddings, where each cluster represents a topic\n",
        "4. Tokenization of Topics – For each cluster, tokenize the original documents to extract words and phrases that occur frequently\n",
        "5. Weight Tokens – Use TF-IDF weighting to score and rank tokens by their importance within each topic\n",
        "6. Represent Topics – Summarize each topic by selecting the top-weighted tokens\n",
        "\n",
        "Clustering:\n",
        "\n",
        "\"BERTopic approaches topic modeling as a cluster task and attempts to cluster semantically similar documents to extract common topics. A disadvantage of using such a method is that [in the default version] each document is assigned to a single cluster and therefore also a single topic. In practice, however documents may conatin.\" (Grootendorst 2024) That is, BERTopic is more a topic clustering (assign 1 topic) than topic modelling approach (assign multiple topics).\n",
        "To assign multiple topics to documents there are several metods using BERTopic (Grootendorst 2024):\n",
        "1. Applying BERTopic on parts of the documents (e.g., sentences).\n",
        "2. Use a cluster model that can perform soft clustering like HDBSCAN.\n",
        "3. Use .approximate_distrbution: \"each document is split into tokens according to the provided tokenizer in the CountVectorizer. Then, a sliding window is applied on each document creating subsets of the document\".\n",
        "\n",
        "Test with the default version have shown: \n",
        "\n",
        "- Assigns only one topic to each document (see above for the reason \n",
        "- Applying the model on raw data leads to non-sensicel results.\n",
        "- The number of topics (around 20) and words per topic (around 20) are small. \n",
        "\n",
        "To use BERTopic in alingment with LDA and NMF, we therefore use a custoized BERTopic model:\n",
        "\n",
        "1. Multiple topics per document: Option 1: Use HDSBCAN (default), which does soft-clustering, and use calculate_probabilities=True (non-default). Option 2: Use k-means (non-default), which does hard clustering, and use .approximate_distrbution.\n",
        "2. Increase number of topics: In HDBSCAN (option 1) the number of clusters/ topics can not be set but indirectly controlled by min_cluster_size; the default is 15, so chose a lower number to increase the number of topics. In k-means we can directly set the number of clusters/ topics to 100. https://maartengr.github.io/BERTopic/getting_started/best_practices/best_practices.html#controlling-number-of-topics, https://maartengr.github.io/BERTopic/getting_started/clustering/clustering.html#k-means\n",
        "3. Increase the words per topic: Use CountVectorizer's parameter max_features to set the number of words per topic. \n",
        "4. Follow LDA and NMF as closely as possible in all other regards: Use the preprocessed data, excl. stopwords, lowercase, exclude to frequent words (max_df=0.5).\n",
        "5. Document-topic and topic-word matrices: Created in analogy to LDA and NMF: https://maartengr.github.io/BERTopic/faq.html#how-do-i-calculate-the-probabilities-of-all-topics-in-a-document, https://maartengr.github.io/BERTopic/getting_started/distribution/distribution.html#example, https://maartengr.github.io/BERTopic/getting_started/tips_and_tricks/tips_and_tricks.html#topic-term-matrix, \n",
        "\n",
        "Randomness:\n",
        "\n",
        "BERTopic is a stochastic model. But its not easy to random seeds in the default version. Thus, we only do it for the final customized version.\n"
      ],
      "metadata": {},
      "id": "c02a6cfc-f4d3-430c-abb7-dd583e13fd27"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Interpretation of the output:\n",
        "\n",
        "- Document-Topic Matrix: Each row represents an earnings call transcript, and each column represents a topic. Originally, the values reflect the strength or weight (based on clustering and the underlying TF-IDF scores) of each topic for that document. After normalization, each row sums to 1, and you can interpret the entries as the probability or proportion of the document that is associated with each topic.\n",
        "\n",
        "- Topic-Word Matrix (BERTopic): Each row corresponds to a topic and each column to a word in the vocabulary. Initially, the values indicate the relative importance of a word within the topic (often derived from TF-IDF weights). Normalizing each row so that it sums to 1 lets you interpret the entries as the probability of a given word appearing in that topic.\n",
        "\n",
        "- Note that BERTopic reserves the label –1 for documents that are considered outliers and are not confidently assigned to any topic."
      ],
      "metadata": {},
      "id": "3aab8758-b20c-458d-acbc-eec4598b7cee"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Helper function"
      ],
      "metadata": {},
      "id": "5fafccaf-b2d8-4a30-bafd-8e49bca4f361"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function\n",
        "def normalize_matrix(df_matrix):\n",
        "    '''\n",
        "    Normalizes a document-topic or topic-word matrix, i.e. makes the row_sum = 1,\n",
        "    i.e. makes the document-topic matrix interpretable as a topic distribution for \n",
        "    each document and the the topic-word matrix as a word distribution for each topic\n",
        "    \n",
        "    Paramters:\n",
        "    - df_matrix: pd.DataFrame, non-normalized document-topic or topic-word matrix\n",
        "\n",
        "    Output:\n",
        "    - pd.DataFrame, normalized document-topic or topic-word matrix\n",
        "    '''\n",
        "    return df_matrix.div(df_matrix.sum(axis=1), axis=0)"
      ],
      "outputs": [],
      "execution_count": 36,
      "metadata": {
        "gather": {
          "logged": 1745073860931
        }
      },
      "id": "88c97506-d37a-4735-a215-bcfa4b68fff1"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function\n",
        "def get_topn_words(df_topic_word, number_top_words):\n",
        "    \"\"\"\n",
        "    Get top n words for each topic from topic-word matrix.\n",
        "\n",
        "    Parameters:\n",
        "    - df_topic_word : pd.DataFrame, topic-word matrix \n",
        "    - number_top_words : int, number of top words to extract\n",
        "\n",
        "    Output:\n",
        "    - list of list: list containing top n words for each topic\n",
        "    \"\"\"\n",
        "    top_words_per_topic = []\n",
        "    for i in range(df_topic_word.shape[0]):\n",
        "        top_words = df_topic_word.iloc[i].nlargest(number_top_words).index.tolist()\n",
        "        top_words_per_topic.append(top_words)\n",
        "    return top_words_per_topic"
      ],
      "outputs": [],
      "execution_count": 37,
      "metadata": {
        "gather": {
          "logged": 1745073862124
        }
      },
      "id": "a4336576-04e7-453a-89fa-aff8021ddc71"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions for the models (incl. Parameterisation)"
      ],
      "metadata": {},
      "id": "168f5e01-81e1-4a8d-9d64-09ae59c5dfca"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### LDA function"
      ],
      "metadata": {},
      "id": "486a5046-7afb-451e-ba47-e1037e06d9a7"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function\n",
        "def lda_fun(dtm, vocab, number_topics, seed):\n",
        "    '''\n",
        "    Fits LDA model and returns model, document-topic and topic-word matrices\n",
        "    \n",
        "    Parameters:\n",
        "    - dtm: np.ndarray, document-term matrix\n",
        "    - number_topics: int, number of topics\n",
        "    - vocab: list of str, feature names/ dtm column names (i.e., the words)\n",
        "    - seed: int, random seed for reproducibility\n",
        "\n",
        "    Outputs:\n",
        "    - lda: fitted LDA model\n",
        "    - df_doc_topic: pd.DataFrame, document-topic distr. matrix (docs x topics)\n",
        "    - df_topic_word: pd.DataFrame, topic-word distr. matrix (topics x words)\n",
        "\n",
        "    Notes:\n",
        "    - output matrices are already normalized (for each row: sum over columns = 1)\n",
        "    '''\n",
        "    # define model\n",
        "    lda = LDA(\n",
        "        n_components=number_topics,\n",
        "        doc_topic_prior=min(50 / number_topics, 1),\n",
        "        topic_word_prior=0.01,\n",
        "        n_jobs=-1,\n",
        "        random_state=seed)\n",
        "    \n",
        "    # Fit model and get document-topic-matrix\n",
        "    m_doc_topic = lda.fit_transform(dtm)\n",
        "    \n",
        "    # Get topic-word-matrix\n",
        "    m_topic_word = lda.components_\n",
        "\n",
        "    # Transform document-topic-matrix and topic-word-matrix to df\n",
        "    df_doc_topic = pd.DataFrame(\n",
        "        m_doc_topic, \n",
        "        columns=[f\"topic_{i}\" for i in range(lda.n_components)])\n",
        "\n",
        "    df_topic_word = pd.DataFrame(\n",
        "        m_topic_word, \n",
        "        columns=vocab)\n",
        "    \n",
        "    return {\"model\": lda, \n",
        "            \"df_doc_topic\": df_doc_topic, \n",
        "            \"df_topic_word\": df_topic_word}"
      ],
      "outputs": [],
      "execution_count": 38,
      "metadata": {
        "gather": {
          "logged": 1745073862895
        }
      },
      "id": "b71119a1-39bc-4935-80ad-79461eac0d54"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### NMF function"
      ],
      "metadata": {},
      "id": "9fd4cf25-8867-4973-9832-04c05aaf5ce3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function\n",
        "def nmf_fun(dtm, vocab, number_topics):\n",
        "    '''\n",
        "    Fits a NMF model and returns model, document-topic and topic-word matrices\n",
        "    \n",
        "    Parameters:\n",
        "    - dtm: np.ndarray, document-term matrix\n",
        "    - number_topics: int, number of topics\n",
        "    - vocab: list of str, feature names/ dtm column names (i.e., the words)\n",
        "\n",
        "    Outputs:\n",
        "    - nmf: fitted LDA model\n",
        "    - df_doc_topic: pd.DataFrame, ocument-topic distr. matrix (docs x topics)\n",
        "    - df_topic_word: pd.DataFrame, topic-word distr. matrix (topics x words)\n",
        "\n",
        "    Notes:\n",
        "    - output matrices are not yet normalized (row sums != 1)\n",
        "    - nmf is deterinistic, i.e. reproducable without seed\n",
        "    '''\n",
        "    # Define model\n",
        "    nmf = NMF(\n",
        "        n_components=number_topics)\n",
        "    \n",
        "    # Fit model and get document-topic-matrix\n",
        "    m_doc_topic = nmf.fit_transform(dtm)\n",
        "    \n",
        "    # Get topic-word-matrix\n",
        "    m_topic_word = nmf.components_\n",
        "\n",
        "    # Transform document-topic-matrix and topic-word-matrix to df\n",
        "    df_doc_topic = pd.DataFrame(\n",
        "        m_doc_topic, \n",
        "        columns=[f\"topic_{i}\" for i in range(nmf.n_components)])\n",
        "\n",
        "    df_topic_word = pd.DataFrame(\n",
        "        m_topic_word, \n",
        "        columns=vocab)\n",
        "    \n",
        "    return {\"model\": nmf,\n",
        "            \"df_doc_topic\": df_doc_topic, \n",
        "            \"df_topic_word\": df_topic_word}"
      ],
      "outputs": [],
      "execution_count": 39,
      "metadata": {
        "gather": {
          "logged": 1745073865191
        }
      },
      "id": "03717e19-425b-4222-ab2c-49b31db0ec9c"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### BERTopic function"
      ],
      "metadata": {},
      "id": "7f4b6cd0-230a-4903-b0dd-89c41d7e82c0"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function\n",
        "def bertopic_fun(docs, number_topics, seed):\n",
        "    '''\n",
        "    Fits a BERTopic model and returns model, document-topic and topic-word matrices\n",
        "\n",
        "    Parameters:\n",
        "    - docs: list of str, input documents\n",
        "    - number_topics: int, number of topics\n",
        "    - seed: int, random seed for reproducibility\n",
        "\n",
        "    Outputs:\n",
        "    - bert: fitted BERTopic model\n",
        "    - df_doc_topic: pd.DataFrame, document-topic distribution (n_docs x n_topics)\n",
        "    - df_topic_word: pd.DataFrame, topic-word distribution (n_topics x words)\n",
        "\n",
        "    Notes:\n",
        "    - Output matrices are not yet normalized (row sums ≠ 1)\n",
        "    '''\n",
        "    # Define model\n",
        "    custom_vectorizer = CountVectorizer(\n",
        "        stop_words=\"english\", \n",
        "        lowercase=True, \n",
        "        max_df=0.5, \n",
        "        max_features=1000)\n",
        "    \n",
        "    umap_model = UMAP( \n",
        "        n_neighbors=15,  \n",
        "        n_components=5, # increases number of dim in dimenson reduction\n",
        "        min_dist=0.1, \n",
        "        metric='cosine',\n",
        "        random_state=seed) \n",
        "    \n",
        "    kmeans_model = KMeans(\n",
        "        n_clusters=number_topics)\n",
        "    \n",
        "    # Create BERTopic model\n",
        "    bert = BERTopic(\n",
        "        # embedding_model=finbert,\n",
        "        vectorizer_model=custom_vectorizer,\n",
        "        umap_model=umap_model,\n",
        "        hdbscan_model=kmeans_model,  #yYes, pass KMeans here!\n",
        "        verbose=True)\n",
        "    \n",
        "    # Fit the model\n",
        "    topics = bert.fit_transform(docs)\n",
        "\n",
        "    # Get topic-word-matrix. \n",
        "    # Note: Approximates document-topic distribution \n",
        "    m_doc_topic, _ = bert.approximate_distribution(docs) \n",
        "    df_doc_topic = pd.DataFrame(\n",
        "        m_doc_topic,\n",
        "        columns=[f\"topic_{i}\" for i in range(m_doc_topic.shape[1])])\n",
        "\n",
        "    # Topic-word matrix\n",
        "    words = bert.vectorizer_model.get_feature_names_out()\n",
        "    df_topic_word = pd.DataFrame(\n",
        "        bert.c_tf_idf_.todense(),\n",
        "        columns=words) \n",
        "    return {\"model\": bert,\n",
        "            \"df_doc_topic\": df_doc_topic, \n",
        "            \"df_topic_word\": df_topic_word}"
      ],
      "outputs": [],
      "execution_count": 40,
      "metadata": {
        "gather": {
          "logged": 1745073865996
        }
      },
      "id": "99375706-5998-4e35-948d-80dcb0e6fd27"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions for evaluating models"
      ],
      "metadata": {},
      "id": "402c06a3-24d7-4ce3-b995-cb9cbf01d6d2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### umass"
      ],
      "metadata": {},
      "id": "5c979455-cbc9-4f40-8be7-cd762147974e"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: The umass coherence score compares the co-occurrence probability of word pairs in the same topic with the probability of these word pairs occurring by chance. The score usually ranges from negative values to zero; numbers closer to zero mean better coherence/ more meaningful topics. Generally, scores around -1 or higher (closer to zero) are considered pretty good"
      ],
      "metadata": {},
      "id": "68e7b379-0428-4d2f-bb49-405ab7ad29b2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function\n",
        "def compute_umass(df_topic_word, dtm, vocab, number_top_words=5):\n",
        "    \"\"\"\n",
        "    Compute mean u_mass coherence using metric_coherence_gensim.\n",
        "    Parameters:\n",
        "    - m_topic_word : pd.DataFrame, topic-word matrix\n",
        "    - dtm : np.ndarray, document-term matrix.\n",
        "    - vocab: list of str, feature names/ dtm column names (i.e., the words)\n",
        "    - number_top_words : int, Number of top words per topic.\n",
        "\n",
        "    Oputput:\n",
        "    - umass: float, mean umass coherence score\n",
        "    - does not require normalized topic-word matrix\n",
        "    \"\"\"\n",
        "    scores = metric_coherence_gensim(\n",
        "        measure=\"u_mass\",\n",
        "        top_n=number_top_words,\n",
        "        topic_word_distrib=df_topic_word.to_numpy(),\n",
        "        dtm=dtm,\n",
        "        vocab=vocab,\n",
        "        texts=None)\n",
        "    \n",
        "    return np.mean(scores)"
      ],
      "outputs": [],
      "execution_count": 41,
      "metadata": {
        "gather": {
          "logged": 1745073868615
        }
      },
      "id": "267a3257-71da-4650-8a96-2ccf7c4688d2"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Topic diversity"
      ],
      "metadata": {},
      "id": "dac54bc9-bd0a-44ca-8ef4-3e11ecbf4b62"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: Topic Diversity measures the ratio of unique top words across all topics to the total number of top words. Scores range from 0 to 1; values closer to 1 indicate that topics share fewer words (i.e. are more distinct and diverse)."
      ],
      "metadata": {},
      "id": "1e970e5c-0c68-47c2-8d8e-5fa946452fa2"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define function\n",
        "def compute_topic_diversity(df_topic_word, number_top_words=5):\n",
        "    \"\"\"\n",
        "    Compute topic diversity\n",
        " \n",
        "    Parameters:\n",
        "    - df_topic_word : pd.DataFrame, topic-word matrix\n",
        "    - number_top_words : int, number of top words per topic.\n",
        "\n",
        "    Output:\n",
        "    - topic diversity: float, topic diversity score\n",
        "\n",
        "    Notes:\n",
        "    - topic diversity = (num. of unique top words across topics) / (total num. of top words)\n",
        "    - does not require normalized topic-word matrix\n",
        "    \"\"\"\n",
        "    # use helper fun to get top words for each topic as list of list\n",
        "    top_words = get_topn_words(df_topic_word, number_top_words) \n",
        "\n",
        "    # Flatten all top words across topics and make list unique\n",
        "    top_words_flattend = []\n",
        "    for topic in top_words:         \n",
        "        for word in topic:          \n",
        "            top_words_flattend.append(word)\n",
        "    \n",
        "    top_words_unique = set(top_words_flattend)  \n",
        "\n",
        "    # calc and return topic diversity\n",
        "    return len(top_words_unique) / (len(top_words) * number_top_words)"
      ],
      "outputs": [],
      "execution_count": 42,
      "metadata": {
        "gather": {
          "logged": 1745073869415
        }
      },
      "id": "e26f46fb-b833-4fc7-886f-5d7b43f84af4"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Intruder analysis"
      ],
      "metadata": {},
      "id": "682d35af-2289-4fa1-9e0d-40afe6a00f9b"
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the functions\n",
        "def intruder_analysis(df_topic_word, vocab, number_top_words=5, seed=1):\n",
        "    \"\"\"\n",
        "    Generate the top words and a random intruder word for each topic in a string\n",
        "\n",
        "    Parameters:\n",
        "    - df_topic_word : pd.DataFrame, topic-word matrix\n",
        "    - vocab: list of str, feature names/ dtm column names (i.e., the words)\n",
        "    - number_top_words : int, Number of top words per topic\n",
        "    - seed: int, random seed for reproducibility\n",
        "\n",
        "    Ouptput:\n",
        "    - str, formatted topic + intruder lines for each topic    \n",
        "    \"\"\"\n",
        "    # Set random seed\n",
        "    random.seed(seed)\n",
        "\n",
        "    # Use helper function to get top words per topic\n",
        "    top_words = get_topn_words(df_topic_word, number_top_words)\n",
        "\n",
        "    # Flatten all top words across topics and make list unique\n",
        "    top_words_flattend = []\n",
        "    for topic in top_words:\n",
        "        for word in topic:\n",
        "            top_words_flattend.append(word)\n",
        "\n",
        "    top_words_unique = set(top_words_flattend)\n",
        "\n",
        "    # collect results as strings\n",
        "    output_lines = []\n",
        "\n",
        "    for i, topic_words in enumerate(top_words):\n",
        "        intr_candidates = [w for w in vocab if w not in topic_words and w not in top_words_unique]\n",
        "        intr_word = random.choice(intr_candidates) if intr_candidates else None\n",
        "        line = f\"Topic {i} words: {'|'.join(topic_words)} | Intruder: {intr_word}\"\n",
        "        output_lines.append(line)\n",
        "\n",
        "    return \"\\n\".join(output_lines)\n"
      ],
      "outputs": [],
      "execution_count": 43,
      "metadata": {
        "gather": {
          "logged": 1745073871636
        }
      },
      "id": "e8cc66b4-07a7-4910-87cd-64fbe9bd178a"
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Calculate and display results"
      ],
      "metadata": {},
      "id": "20b923ee-fb06-46fa-9476-fea5a8c87519"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Loop (incl. possibility for tuning topic numbers)"
      ],
      "metadata": {},
      "id": "58d8fd7f-a0c7-4cbf-9163-9649e302bf85"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Implementation notes: (1)  dtm_tf and dtm_tfidf are unequal while vocab_tf and vocab_tfidf are equal. (2) For computing umass, one should use the same raw count-based DTM (dtm_tf) incl. the corresponding vocabulary vocab_tf for LDA, NMF, and BERTopic. (3) For intruder analysis, one should use the same vorabluary (vocab_tf) for LDA, NMF, and BERTopic. (4) In lda_fun and mnf_fun function calls we use vobab_tf and vocab_tfidf (eventhough they are identical) because of consistency with the parameters dtm_tf and dtm_tfidf (which are not identical)\n"
      ],
      "metadata": {},
      "id": "f7f4eef3-d249-4df1-8245-35c334e4ac4b"
    },
    {
      "cell_type": "code",
      "source": [
        "# Specify number of topics\n",
        "L_number_topics = [100]"
      ],
      "outputs": [],
      "execution_count": 44,
      "metadata": {
        "gather": {
          "logged": 1745073872737
        }
      },
      "id": "6a57e27e-cede-405a-80cb-d7d5578b3685"
    },
    {
      "cell_type": "code",
      "source": [
        "# Prep data for BERTopic as list\n",
        "docs = df[\"text_clean\"].tolist()"
      ],
      "outputs": [],
      "execution_count": 45,
      "metadata": {
        "gather": {
          "logged": 1745073874289
        }
      },
      "id": "f941ab64-a653-46d5-92a1-a45e5f85b2f3"
    },
    {
      "cell_type": "code",
      "source": [
        "# Loop: Fit models and get quantiative and qualitative measures\n",
        "dict_results = {}\n",
        "\n",
        "for number_topics in L_number_topics:\n",
        "\n",
        "    print(\"number_topics:\", number_topics)\n",
        "    \n",
        "    # LDA\n",
        "    out_lda = lda_fun(dtm=dtm_tf, vocab=vocab_tf, number_topics=number_topics, seed=seed)\n",
        "    umass_lda = compute_umass(df_topic_word=out_lda[\"df_topic_word\"], dtm=dtm_tf, vocab=vocab_tf, number_top_words=5)\n",
        "    topic_div_lda = compute_topic_diversity(df_topic_word=out_lda[\"df_topic_word\"], number_top_words=5)\n",
        "    intruder_lda = intruder_analysis(df_topic_word=out_lda[\"df_topic_word\"], vocab=vocab_tf, number_top_words=5, seed=seed)\n",
        "    dict_results[f\"LDA_{number_topics}\"] = {\"df_doc_topic\": out_lda[\"df_doc_topic\"], \"df_topic_word\": out_lda[\"df_topic_word\"], \"umass\": umass_lda, \"topic_diversity\": topic_div_lda, \"intruder\": intruder_lda}\n",
        "\n",
        "    # NMF\n",
        "    # out_nmf = nmf_fun(dtm=dtm_tfidf, vocab=vocab_tfidf, number_topics=number_topics)\n",
        "    # umass_nmf = compute_umass(df_topic_word=out_nmf[\"df_topic_word\"], dtm=dtm_tf, vocab=vocab_tf, number_top_words=5)\n",
        "    # topic_div_nmf = compute_topic_diversity(df_topic_word=out_nmf[\"df_topic_word\"], number_top_words=5)\n",
        "    # intruder_nmf = intruder_analysis(df_topic_word=out_nmf[\"df_topic_word\"], vocab=vocab_tf, number_top_words=5, seed=seed)\n",
        "    # dict_results[f\"NMF_{number_topics}\"] = {\"df_doc_topic\": out_nmf[\"df_doc_topic\"], \"df_topic_word\": out_nmf[\"df_topic_word\"], \"umass\": umass_nmf, \"topic_diversity\": topic_div_nmf, \"intruder\": intruder_nmf}\n",
        "\n",
        "    # # BERTopic\n",
        "    # out_bert = bertopic_fun(docs=docs, number_topics=number_topics, seed=seed)\n",
        "    # umass_bert = compute_umass(df_topic_word=out_bert[\"df_topic_word\"], dtm=dtm_tf, vocab=vocab_tf, number_top_words=5)\n",
        "    # topic_div_bert = compute_topic_diversity(df_topic_word=out_bert[\"df_topic_word\"], number_top_words=5)\n",
        "    # intruder_bert = intruder_analysis(df_topic_word=out_bert[\"df_topic_word\"], vocab=vocab_tf, number_top_words=5, seed=seed)\n",
        "    # dict_results[f\"BERTopic_{number_topics}\"] = {\"df_doc_topic\": out_bert[\"df_doc_topic\"], \"df_topic_word\": out_bert[\"df_topic_word\"], \"umass\": umass_bert, \"topic_diversity\": topic_div_bert, \"intruder\": intruder_bert}"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "number_topics: 100\n"
        }
      ],
      "execution_count": 46,
      "metadata": {
        "gather": {
          "logged": 1745074129850
        }
      },
      "id": "899e11f8-8d4c-4006-96f5-12f4dde5cfea"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Quantiative results"
      ],
      "metadata": {},
      "id": "81888e01-4d12-43b5-a8b7-97e89707bc7e"
    },
    {
      "cell_type": "code",
      "source": [
        "# Create summary table from dict_results\n",
        "summary_rows = []\n",
        "\n",
        "for label, result in dict_results.items():\n",
        "    summary_rows.append({\n",
        "        \"model_number_topics\": label,\n",
        "        \"umass\": result[\"umass\"],\n",
        "        \"topic_diversity\": result[\"topic_diversity\"]})\n",
        "\n",
        "df_summary = pd.DataFrame(summary_rows)\n",
        "print(df_summary)"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "  model_number_topics     umass  topic_diversity\n0             LDA_100 -0.977191            0.664\n"
        }
      ],
      "execution_count": 47,
      "metadata": {
        "gather": {
          "logged": 1745074132292
        }
      },
      "id": "bb39ea9f-dbba-417c-a20f-065e2984cea8"
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Qualitative results"
      ],
      "metadata": {},
      "id": "c503e96b-0ee9-4a3d-a0ea-899d806da31d"
    },
    {
      "cell_type": "code",
      "source": [
        "# print the intruder analysis from dict_results\n",
        "for label, result in dict_results.items():\n",
        "    print(f\"{label} intruder analysis\")\n",
        "    print(result[\"intruder\"])\n",
        "    print()"
      ],
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": "LDA_100 intruder analysis\nTopic 0 words: store|comp|online|traffic|retail | Intruder: deutsche\nTopic 1 words: tool|new product|currency|critical|unfavorable | Intruder: thing like\nTopic 2 words: organic|organic growth|organic revenue|services|organically | Intruder: cetera\nTopic 3 words: senior|senior vice|yeah|markets|capital markets | Intruder: hope\nTopic 4 words: brand|retail|channel|wholesale|store | Intruder: cowen\nTopic 5 words: aircraft|fleet|united|delivery|schedule | Intruder: sale increase\nTopic 6 words: cloud|enterprise|data|datum center|provider | Intruder: quarter also\nTopic 7 words: currency|constant|constant currency|new product|americas | Intruder: reform\nTopic 8 words: loan|deposit|net interest|ratio|fee | Intruder: year year\nTopic 9 words: plant|packaging|ton|export|brazil | Intruder: offering\nTopic 10 words: gross margin|shipment|new product|ship|manufacturing | Intruder: finish\nTopic 11 words: marketing|user|app|mobile|advertising | Intruder: consolidated\nTopic 12 words: land|community|housing|california|construction | Intruder: revenue million\nTopic 13 words: test|testing|recur|reference|practice | Intruder: apply\nTopic 14 words: us|raw|raw material|inflation|price increase | Intruder: operator operator\nTopic 15 words: gas|energy|natural|natural gas|plant | Intruder: profitability\nTopic 16 words: air|fleet|ebitda|capex|turn slide | Intruder: upside\nTopic 17 words: brand|innovation|marketing|retailer|saving | Intruder: accounting\nTopic 18 words: device|currency|office|constant|constant currency | Intruder: pull\nTopic 19 words: net sale|market share|profit|retail|currency | Intruder: income million\nTopic 20 words: digital|innovation|connect|engagement|app | Intruder: geographic\nTopic 21 words: fuel|stuff|card|macro|payment | Intruder: trajectory\nTopic 22 words: agreement|quarter million|diluted|compare million|net income | Intruder: continue invest\nTopic 23 words: ai|design|software|data|innovation | Intruder: low end\nTopic 24 words: oil|production|barrel|per day|basin | Intruder: approval\nTopic 25 words: estate|real estate|ratio|market share|yield | Intruder: among\nTopic 26 words: chairman|chairman chief|chairman president|president chief|llc | Intruder: announcement\nTopic 27 words: fund|fee|equity|client|private | Intruder: year see\nTopic 28 words: covid|vaccine|pandemic|recovery|test | Intruder: strengthen\nTopic 29 words: profit|engine|energy|backlog|equipment | Intruder: adjust earning\nTopic 30 words: digital|news|advertising|australia|ebitda | Intruder: old\nTopic 31 words: indiscernible|recovery|money|government|stuff | Intruder: footprint\nTopic 32 words: rating|finance|percent|bond|incentive | Intruder: present\nTopic 33 words: claim|reserve|premium|individual|care | Intruder: apply\nTopic 34 words: premium|ratio|insurance|property|book | Intruder: sound like\nTopic 35 words: equipment|practice|local|currency|internal | Intruder: franchise\nTopic 36 words: trading|trade|option|index|client | Intruder: promotional\nTopic 37 words: pandemic|recovery|march|liquidity|safety | Intruder: sale increase\nTopic 38 words: york|new york|water|city|american | Intruder: sustain\nTopic 39 words: pro|season|net revenue|online|block | Intruder: get lot\nTopic 40 words: payment|card|merchant|partnership|digital | Intruder: minute\nTopic 41 words: guest|member|barrel|heavy|capture | Intruder: get back\nTopic 42 words: rent|lease|property|occupancy|cap | Intruder: foundation\nTopic 43 words: japan|hedge|sector|ratio|policy | Intruder: quarter see\nTopic 44 words: energy|utility|electric|transmission|gas | Intruder: ladies\nTopic 45 words: hotel|room|property|fee|brand | Intruder: among\nTopic 46 words: site|carrier|cell|network|deployment | Intruder: planning\nTopic 47 words: tariff|trade|marketplace|list|price increase | Intruder: switch\nTopic 48 words: client|fee|wealth|deposit|banking | Intruder: would think\nTopic 49 words: president chief|director|executive vice|broker|llc | Intruder: continue focus\nTopic 50 words: chain|supply chain|constraint|transportation|automation | Intruder: exciting\nTopic 51 words: specialty|wells|brazil|volume growth|hedge | Intruder: well position\nTopic 52 words: automotive|industrial|auto|mobile|application | Intruder: later year\nTopic 53 words: division|research division|llc|capital markets|markets | Intruder: culture\nTopic 54 words: insurance|analytic|subscription|transition|energy | Intruder: meeting\nTopic 55 words: backlog|award|budget|booking|operate margin | Intruder: seasonality\nTopic 56 words: storage|internal|recall|street|come line | Intruder: present\nTopic 57 words: industrial|oil|oil gas|gas|end market | Intruder: see lot\nTopic 58 words: weather|march|season|february|freight | Intruder: expenditure\nTopic 59 words: buyer|active|option|land|community | Intruder: let start\nTopic 60 words: foot|square|lease|square foot|building | Intruder: isi\nTopic 61 words: care|health|patient|hospital|facility | Intruder: tough\nTopic 62 words: proceed|proceed question|please proceed|come line|question line | Intruder: seasonal\nTopic 63 words: yield|ship|book|brand|fuel | Intruder: see good\nTopic 64 words: food|restaurant|delivery|comp|labor | Intruder: original\nTopic 65 words: synergy|integration|legacy|integrate|saving | Intruder: traction\nTopic 66 words: security|software|application|enterprise|infrastructure | Intruder: ask question\nTopic 67 words: digit|china|single digit|double digit|life | Intruder: requirement\nTopic 68 words: production|ramp|delivery|government|safety | Intruder: got\nTopic 69 words: water|residential|inflation|productivity|americas | Intruder: partners\nTopic 70 words: patient|study|phase|trial|therapy | Intruder: personal\nTopic 71 words: fiscal|profit|gross profit|brand|fiscal year | Intruder: enter\nTopic 72 words: booking|travel|book|direct|recovery | Intruder: next month\nTopic 73 words: wireless|network|mobile|phone|ebitda | Intruder: successfully\nTopic 74 words: procedure|hospital|patient|clinical|adoption | Intruder: objective\nTopic 75 words: organic|restructuring|organic growth|operate margin|adjust operate | Intruder: competition\nTopic 76 words: vehicle|truck|dealer|china|production | Intruder: proposition\nTopic 77 words: emerge|emerge market|innovation|china|latin | Intruder: see strong\nTopic 78 words: maintenance|special|south|share repurchase|export | Intruder: convert\nTopic 79 words: network|video|marketplace|mobile|connect | Intruder: eight\nTopic 80 words: fiscal|fiscal year|quarter fiscal|calendar|diluted | Intruder: smart\nTopic 81 words: auto|car|policy|ratio|direct | Intruder: original\nTopic 82 words: home|gross margin|incentive|housing|community | Intruder: north american\nTopic 83 words: international|domestic|network|profit|peak | Intruder: road\nTopic 84 words: healthcare|medical|health|care|member | Intruder: appropriate\nTopic 85 words: ebitda|adjust ebitda|ebitda margin|venture|joint | Intruder: ready\nTopic 86 words: inflation|productivity|gross margin|saving|innovation | Intruder: barclays\nTopic 87 words: content|game|player|sport|mobile | Intruder: limit\nTopic 88 words: rig|international|land|fleet|equipment | Intruder: vertical\nTopic 89 words: exchange|foreign|foreign exchange|sequentially|sequential | Intruder: treatment\nTopic 90 words: life|retirement|interest rate|ratio|variable | Intruder: three year\nTopic 91 words: llc|securities llc|markets|yeah|lynch | Intruder: otherwise\nTopic 92 words: gross margin|sequentially|ramp|china|design | Intruder: year go\nTopic 93 words: china|take next|operator take|december|september | Intruder: engage\nTopic 94 words: train|network|question line|productivity|mexico | Intruder: encourage\nTopic 95 words: express|ground|package|peak|delivery | Intruder: secondly\nTopic 96 words: solutions|mortgage|transformation|new product|services | Intruder: gap\nTopic 97 words: come line|organization|feel good|back half|really good | Intruder: adjustment\nTopic 98 words: client|retention|new business|agency|talent | Intruder: feature\nTopic 99 words: construction|texas|aggregate|infrastructure|california | Intruder: steady\n\n"
        }
      ],
      "execution_count": 48,
      "metadata": {
        "gather": {
          "logged": 1745074133336
        }
      },
      "id": "cb4678e9-eba4-43f0-9f2d-a0f4133d02ee"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Output"
      ],
      "metadata": {},
      "id": "d67bad2c-4a4f-4be1-b0f6-d456e8b950d0"
    },
    {
      "cell_type": "markdown",
      "source": [
        "Note: In this section the document-topic and topic-word matrices are saved as csv"
      ],
      "metadata": {
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "id": "7f73f1d9-4006-41d3-bb24-96773f0e59fe"
    },
    {
      "cell_type": "code",
      "source": [
        "# Ensure metadata is clean and aligned\n",
        "df_valid = df[df[\"text_clean_lemma\"].notna()].reset_index(drop=True)\n",
        "df_metadata = df_valid.drop(columns=[\"text\", \"text_clean\", \"text_clean_lemma\"]).reset_index(drop=True)\n",
        "\n",
        "# Save normalized matrices with consistent and clear naming\n",
        "for model_name, result in dict_results.items():\n",
        "    # Normalize both matrices\n",
        "    df_doc_topic_norm = normalize_matrix(result[\"df_doc_topic\"]).reset_index(drop=True)\n",
        "    df_topic_word_norm = normalize_matrix(result[\"df_topic_word\"])\n",
        "\n",
        "    # Augment the document-topic matrix with metadata\n",
        "    df_doc_topic_norm_augmented = pd.concat([df_metadata, df_doc_topic_norm], axis=1)\n",
        "\n",
        "    # Save both matrices\n",
        "    df_doc_topic_norm_augmented.to_csv(f\"{model_name}_doc_topic_norm_augmented.csv\", index=False)\n",
        "    df_topic_word_norm.to_csv(f\"{model_name}_topic_word_norm.csv\", index=False)"
      ],
      "outputs": [],
      "execution_count": 49,
      "metadata": {
        "gather": {
          "logged": 1745074135175
        }
      },
      "id": "108864f0-3da3-4887-bb4e-c8fe4300f0bc"
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": null,
      "metadata": {},
      "id": "d348670e-5022-439f-94dc-a1d8bad75ae1"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "clean38",
      "language": "python",
      "display_name": "Python (clean38)"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.20",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "microsoft": {
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      },
      "host": {
        "AzureML": {
          "notebookHasBeenCompleted": true
        }
      }
    },
    "kernel_info": {
      "name": "clean38"
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}